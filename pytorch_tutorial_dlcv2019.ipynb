{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tutorial_dlcv2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhewei-forks/pytorch-tutorial/blob/master/pytorch_tutorial_dlcv2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-Zsi2fSrBNHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pytorch Practice: Classifying MNIST images"
      ]
    },
    {
      "metadata": {
        "id": "gnIS5T5wC0w2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this example, we will teach how to use Pytorch for a classic CV task: Classifying hand-written digits.  \n",
        "We will cover the basics of Pytorch, including:\n",
        "\n",
        "\n",
        "*   How to create an neural network\n",
        "*   How to create loader for custom data\n",
        "*   How to train a neural network\n",
        "*   How to save the model\n",
        "*   How to fine-tune a pre-trained model\n",
        "*   How to debug the model\n",
        "\n",
        "Reference: [CS231n](http://cs231n.stanford.edu/notebooks/pytorch_tutorial.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "X8-3SvzotsSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download MNIST data"
      ]
    },
    {
      "metadata": {
        "id": "b3fpMVP3trbe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! wget -O mnist_png.tar.gz https://github.com/myleott/mnist_png/blob/master/mnist_png.tar.gz?raw=true\n",
        "! tar zxf mnist_png.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fFfHJM7QCi6H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Pytorch module"
      ]
    },
    {
      "metadata": {
        "id": "VCsa3-HSBHce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4M-ON6BCttO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Creating a custom dataset\n",
        "\n",
        "PyTorch has many built-in datasets such as MNIST and CIFAR.  \n",
        "In this tutorial, we demonstrate how to write your own dataset by implementing a custom MNIST dataset class."
      ]
    },
    {
      "metadata": {
        "id": "yLQQyUuiELen",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
        "        self.images = None\n",
        "        self.labels = None\n",
        "        self.filenames = []\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "\n",
        "        # read filenames\n",
        "        for i in range(10):\n",
        "            filenames = glob.glob(os.path.join(root, str(i), '*.png'))\n",
        "            for fn in filenames:\n",
        "                self.filenames.append((fn, i)) # (filename, label) pair\n",
        "                \n",
        "        self.len = len(self.filenames)\n",
        "                              \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Get a sample from the dataset \"\"\"\n",
        "        image_fn, label = self.filenames[index]\n",
        "        image = Image.open(image_fn)\n",
        "            \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Total number of samples in the dataset \"\"\"\n",
        "        return self.len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lIUahJADuzLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's load the images into custom-created Dataset."
      ]
    },
    {
      "metadata": {
        "id": "pBMQwR0oEetn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the MNIST dataset. \n",
        "# transforms.ToTensor() automatically converts PIL images to\n",
        "# torch tensors with range [0, 1]\n",
        "trainset = MNIST(root='mnist_png/training',\n",
        "    transform=transforms.ToTensor())\n",
        "\n",
        "# load the testset\n",
        "testset = MNIST(root='mnist_png/testing',\n",
        "    transform=transforms.ToTensor())\n",
        "\n",
        "print('# images in trainset:', len(trainset)) # Should print 60000\n",
        "print('# images in testset:', len(testset)) # Should print 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2t1NpnH4xe8y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In Pytorch, the \"DataLoader\" class provides a simple way to collect data into batches."
      ]
    },
    {
      "metadata": {
        "id": "GjQngQ6fvU9k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use the torch dataloader to iterate through the dataset\n",
        "trainset_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
        "testset_loader = DataLoader(testset, batch_size=1000, shuffle=False, num_workers=1)\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainset_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print('Image tensor in each batch:', images.shape, images.dtype)\n",
        "print('Label tensor in each batch:', labels.shape, labels.dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Mm4HWGJC7mW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can visualize what contains in each batch:"
      ]
    },
    {
      "metadata": {
        "id": "IrfTpBYeC6Oa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    \n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print('Labels:')\n",
        "print(' '.join('%5s' % labels[j] for j in range(16)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBkiKKd7ywy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check if GPU is available. If not, use CPU instead."
      ]
    },
    {
      "metadata": {
        "id": "6BRAp_-sEk1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use GPU if available, otherwise stick with cpu\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(123)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Device used:', device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cUqSqPmVEmTv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Creating a Convolutional Neural Network\n",
        "\n",
        "Pytorch uses \"torch.nn.Module()\" to wrap a neural network into a class object."
      ]
    },
    {
      "metadata": {
        "id": "0txCMNSBEs6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(10, 20, kernel_size=5),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(320, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = Net().to(device) # Remember to move the model to \"device\"\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "StihLWC4EvfH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Train the network\n",
        "\n",
        "With the data loaded and network created, it's time to train the model!  \n",
        "First, we define the training loop."
      ]
    },
    {
      "metadata": {
        "id": "6gp-5kTUExjg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, epoch, log_interval=100):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()  # Important: set training mode\n",
        "    \n",
        "    iteration = 0\n",
        "    for ep in range(epoch):\n",
        "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if iteration % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
        "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
        "            iteration += 1\n",
        "            \n",
        "        test(model) # Evaluate at the end of each epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJtDTy6F_4O-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Remember to evaluate at the end of each epoch."
      ]
    },
    {
      "metadata": {
        "id": "5jqywjrCE1H3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()  # Important: set evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # This will free the GPU memory used for back-prop\n",
        "        for data, target in testset_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(testset_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(testset_loader.dataset),\n",
        "        100. * correct / len(testset_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edzYXXF0__RH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's time to train the model!"
      ]
    },
    {
      "metadata": {
        "id": "w7HNFf5eE2dY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(model, 5)  # train 5 epochs should get you to about 97% accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXZxtDy-5FdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Save the model\n",
        "Now we have a model! Obviously we do not want to retrain the model everytime we want to use it. Plus if you are training a super big model, you probably want to save checkpoint periodically so that you can always fall back to the last checkpoint in case something bad happened or you simply want to test models at different training iterations.\n",
        "\n",
        "Model checkpointing is fairly simple in PyTorch. First, we define a helper function that can save a model to the disk."
      ]
    },
    {
      "metadata": {
        "id": "qxPSUrN15rcd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = {'state_dict': model.state_dict(),\n",
        "             'optimizer' : optimizer.state_dict()}\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print('model saved to %s' % checkpoint_path)\n",
        "    \n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    optimizer.load_state_dict(state['optimizer'])\n",
        "    print('model loaded from %s' % checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCooXa7k6yfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define a training loop with model checkpointing:"
      ]
    },
    {
      "metadata": {
        "id": "9R_sCj6r68QZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_save(model, epoch, save_interval, log_interval=100):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()  # set training mode\n",
        "    \n",
        "    iteration = 0\n",
        "    for ep in range(epoch):\n",
        "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if iteration % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
        "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
        "            if iteration % save_interval == 0 and iteration > 0:\n",
        "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
        "            iteration += 1\n",
        "        test(model)\n",
        "    \n",
        "    # save the final model\n",
        "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_AHraUa47rHo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can save the model in each iteration of training."
      ]
    },
    {
      "metadata": {
        "id": "gu5L0qVh7HuE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a brand new model\n",
        "model = Net().to(device)\n",
        "test(model)\n",
        "train_save(model, 5, 500, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yfJ0bbtn7dJd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Assume that we have stopped our training program.  \n",
        "To load the saved model, we need to create the model and optimizer once again.  \n",
        "Then we load the model weight and optimizer."
      ]
    },
    {
      "metadata": {
        "id": "SqmSVq4Z7FVn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a new model\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# load from the final checkpoint\n",
        "load_checkpoint('mnist-4690.pth', model, optimizer)\n",
        "\n",
        "# should give you the final model accuracy\n",
        "test(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9WARTXlU9dSK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Fine-tuning a pre-trained model\n",
        "\n",
        "Sometimes you want to fine-tune a pretrained model instead of training a model from scratch. For example, if you want to train a model on a new dataset that contains natural images. To achieve the best performance, you can start with a model that's fully trained on ImageNet and fine-tune the model.\n",
        "\n",
        "Finetuning a model in PyTorch is super easy! First, let's find out what we saved in a checkpoint:"
      ]
    },
    {
      "metadata": {
        "id": "w1Noa3nO9uSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# What's in a state dict?\n",
        "print(model.state_dict().keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bK5Y3zqW93SW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now say we want to load the conv layers from the checkpoint and train the fc layers. We can simply load a subset of the state dict with the selected names"
      ]
    },
    {
      "metadata": {
        "id": "ddMw3ywB99aF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('mnist-4690.pth')\n",
        "states_to_load = {}\n",
        "for name, param in checkpoint['state_dict'].items():\n",
        "    if name.startswith('conv'):\n",
        "        states_to_load[name] = param\n",
        "\n",
        "# Construct a new state dict in which the layers we want\n",
        "# to import from the checkpoint is update with the parameters\n",
        "# from the checkpoint\n",
        "model_state = model.state_dict()\n",
        "model_state.update(states_to_load)\n",
        "        \n",
        "model = Net().to(device)\n",
        "model.load_state_dict(model_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KIcovVOb-ODO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see how is the fine-tuning result."
      ]
    },
    {
      "metadata": {
        "id": "ulPQ-ff2-Igx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(model, 1)  # training 1 epoch will get you to 93%!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWWcoiOW-5C5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can even use the pretrained conv layers in a different model."
      ]
    },
    {
      "metadata": {
        "id": "AoW1yJ_D-52V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SmallNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SmallNet, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(10, 20, kernel_size=5),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc1 = nn.Linear(320, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "model = SmallNet().to(device)\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNYuZzh9--PT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('mnist-4690.pth')\n",
        "states_to_load = {}\n",
        "for name, param in checkpoint['state_dict'].items():\n",
        "    if name.startswith('conv'):\n",
        "        states_to_load[name] = param\n",
        "\n",
        "# Construct a new state dict in which the layers we want\n",
        "# to import from the checkpoint is update with the parameters\n",
        "# from the checkpoint\n",
        "model_state = model.state_dict()\n",
        "model_state.update(states_to_load)\n",
        "        \n",
        "model.load_state_dict(model_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sqqp4ZAG-_hV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(model, 1)  # training 1 epoch will get you to 93%!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55zSQ4nZBjV3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. How to debug the model?\n",
        "\n",
        "Debugging in Pytorch is quite simple.  \n",
        "**\"Check your tensor shape and data type constantly.\"**  \n",
        "For example, if we want to visualize the tensor blob when forwarding the model:"
      ]
    },
    {
      "metadata": {
        "id": "Zid2H3A1CF8D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(10, 20, kernel_size=5),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(320, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        print('Tensor size and type after conv1:', x.shape, x.dtype)\n",
        "        x = self.conv2(x)\n",
        "        print('Tensor size and type after conv2:', x.shape, x.dtype)\n",
        "        x = x.view(-1, 320)\n",
        "        print('Tensor size and type after view():', x.shape, x.dtype)\n",
        "        x = self.fc1(x)\n",
        "        print('Tensor size and type after fc1:', x.shape, x.dtype)\n",
        "        x = self.fc2(x)\n",
        "        print('Tensor size and type after fc2:', x.shape, x.dtype)\n",
        "        return x\n",
        "\n",
        "model = Net().to(device) # Remember to move the model to \"device\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sAGJheVvCp84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fowarding a dummy tensor\n",
        "x = torch.Tensor(64,1,28,28).to(device) # shape of N*C*H*W\n",
        "x = model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}